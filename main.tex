%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A template for Wiley article submissions.
% Developed by Overleaf. 
%
% Please note that whilst this template provides a 
% preview of the typeset manuscript for submission, it 
% will not necessarily be the final publication layout.
%
% Usage notes:
% The "blind" option will make anonymous all author, affiliation, correspondence and funding information.
% Use "num-refs" option for numerical citation and references style.
% Use "alpha-refs" option for author-year citation and references style.

%\documentclass[num-refs]{wiley-article}
\documentclass[...,numrefs]{wiley-article}

% Add additional packages here if required
\usepackage{natbib}
\usepackage{siunitx}
\usepackage{rotating}
\usepackage{relsize}
\usepackage{amsmath}
\usepackage{subcaption}



% Update article type if known
\papertype{Original Article}
% Include section in journal if known, otherwise delete
\paperfield{Journal Section}

\title{FYS-STK Project 1 on Machine Learning}
%\noindent\shorttitle{Performance and wake of a VAWT for different conditions}


% List abbreviations here, if any. Please note that it is preferred that abbreviations be defined at the first instance they appear in the text, rather than creating an abbreviations list.
%\abbrevs{HAWT, horizontal axis wind turbine; VAWT, vertical axis wind turbine}

% Include full author names and degrees, when required by the journal.
% Use the \authfn to add symbols for additional footnotes and present addresses, if any. Usually start with 1 for notes about author contributions; then continuing with 2 etc if any author has a different present address.
\author{Alexander D. Aliferis}
\author{Marius Stette Jessen}

% Include full affiliation details for all authors

\affil{Department of Physics, University of Oslo, Norway}


%\corraddress{R. Jason Hearst, Energy and Process Engineering, Norwegian University of Science and Technology, Trondheim, NO-7491, Norway}
\corremail{alexadal@fys.uio.no}

% Include the name of the author that should appear in the running header
\runningauthor{D. Aliferis, S. Jessen}

\begin{document}

\maketitle

\begin{abstract}
This is a generic template designed for use by multiple journals, which includes several options for customization. Please consult the author guidelines for the journal to which you are submitting in order to confirm that your manuscript will comply with the journal's requirements. Please replace this text with your abstract.

% Please include a maximum of seven keywords
\keywords{Ordinary Least Squares, Ridge, Lasso, Machine Learning}
\end{abstract}
\newline
\section{Introduction}
In terms of modern science and general data analysis, there are huge efforts in finding patterns and making predictions based on historical observations. These efforts go across a large variety of disciplines and their results have already made significant impact on politics, industry, healthcare and trading to mention some. An ever more trending expression these days, describing the process of predicting certain outcomes based on obtained data, without necessarily giving specific sets of instructions, is Machine Learning. The many sets of algorithms that build up under the subject of Machine Learning, can be divided into two main categories - supervised and unsupervised learning. Within the category of supervised learning, regression models are widely used, and one of the most common subgroup of these is linear regression. This paper will focus on the implementation of three different types of linear regression models, namely ordinary least squares, ridge and lasso regression. Their corresponding algorithms will be used to fit the true Franke function generated by random data input, as well as real terrain data from the coastline of Norway. The study will further assess how well the different models perform in terms statistical properties such as mean squared error, $R^2$-score and how interpretability of the models varies with flexibility. 

\vspace{22mm}

\section{Methodology}
\subsection{Linear regression models}

Regression models make predictions on the response value $Z$, based on new inputs $\boldsymbol{X}$, and a function $ f(\boldsymbol{X})$. This relationship is given by equation \ref{fx_Z}

\begin{equation}
    Z =  f(\boldsymbol{X})+\epsilon
    \label{fx_Z}
\end{equation}

where $\epsilon$ is the irreducible error and represent measurement errors and other discrepancies. The function $f(\boldsymbol{X})$ itself, requires by statistical decision theory a punishment for prediction-errors in the form a loss function, $L(Z,f(\boldsymbol{X}))$. The most common loss function is the mean squared error loss, $L(Z,f(\boldsymbol{X})) = (Z-f(\boldsymbol{X}))^2$, and the function that minimizes this loss is called the regression function $f(\boldsymbol{X})= E(Z\vert{X=x,Y=y ...})$. With limited amounts of data points however, it is seldom possible to compute the exact expectation of $Z$ at any given point. Instead, the function is relaxed and approximated. A good and easily interpretable approximation is the linear regression function, where the ideal function is parameterized by $p+1$ parameters $\beta = [\beta_{0}, \beta_{1}, ... \beta_{p}]$

\begin{equation}
    f(\boldsymbol{X}) = \beta_{0}+\beta_{1}\boldsymbol{X_{1}}+\beta_{2}\boldsymbol{X_{2}}+ ... + \beta_{p}\boldsymbol{X_{p}}
    \label{fx_beta}
\end{equation}

or in matrix representation

\begin{equation}
    f(\boldsymbol{X}) = \boldsymbol{X}\beta
\end{equation}

Here, $\boldsymbol{X_{1}},\boldsymbol{X_{2}}, .., \boldsymbol{X_{p}}$ are vectors and column elements of the design matrix, $\boldsymbol{X}$ ,which is a $N\times{p}$ dimensional matrix. N are the numbers of observations or measured values and p are the characteristics of the $N^{th}$ observation. The number of parameters in the design matrix depends on the complexity, or in other words, the polynomial degree of the regression model. In the case of this paper, where the predictions are based on two independent variables, the number of parameters $p = (\left(degrees+1)\times(degrees+2)\right)/2$.

By using the dependencies above, one can obtain estimated values of the parameters or coefficients, $\hat{\beta}$, and compute predictions on new sets of observations $\Tilde{Z}$. There are several ways of optimizing these coefficients, and perhaps some of the most popular are given in the subsequent sections.


\subsubsection{Ordinary Least Squares, OLS}

The squared sum of residuals, RSS, is defined by taking the spread of each observation-prediction pairs, $z_i-\Tilde{z_i}$, and squaring the sum of these pairs

\begin{equation}
    RSS_i = \sum_{i}^{p}(z_i - \Tilde{z}_{i})^{2}
\end{equation}

Similarly, one can obtain equation \ref{Rss} by using the equality $\Tilde{Z}=\hat{f}(\boldsymbol{X}) = \boldsymbol{X}\hat{\beta}$, equation \ref{fx_beta} and matrix notation

\begin{equation}
    RSS(\beta) = 
     (Z-\boldsymbol{X}\hat{\beta})^T(Z-\hat{\beta}\boldsymbol{X})
     \label{Rss}
\end{equation}
 
Minimizing equation \ref{Rss} by taking its derivative with respect to $\hat{\beta}$, gives the unique solution 

\begin{equation}
    \hat{\beta} = (\boldsymbol{X}^{T}\boldsymbol{X})^{-1}\boldsymbol{X}^{T}Z
    \label{OLS_beta}
\end{equation}

The expression in \ref{OLS_beta} can, if $(\boldsymbol{X}^{T}\boldsymbol{X})^{-1}$ is non-singular and invertible, be used to evaluate $\Tilde{Z}$. 
\newline
\newline
\noindent OLS is a powerful tool that often give good and easily interpretable predictions with a relatively "simple" estimation procedure. However, the model has some drawbacks and limitations. In particular, if the number of parameters become very large and $p>N$, the design matrix cannot have linearly independent columns, and the expression $\boldsymbol{X}^{T}\boldsymbol{X}$ in equation \ref{OLS_beta} is not invertible. OLS is also sensitive to extreme observations or outliers. This is a problem that rise from the cost function itself. For extreme observations, the coefficients in the model will become large, minimizing the distance between the regression line and the outliers. Hence, outliers will affect the slope of the regression line to a greater extent than observations near the mean value of the data set. 
\newline



\noindent To mitigate these negative effects, one can use shrinkage methods that modify the OLS model. Two examples of these are the Ridge and Lasso models that penalize the coefficients for becoming too large, and tries to shrink them towards zero. 


\subsubsection{Ridge Regression}
Similarly to OLS, Ridge regression tries to minimize the the distance between the regression line and the data set. However, it also applies a shrinkage method that results in the following cost function

\begin{equation}
    RSS + \lambda\sum_{j}^{p}\beta_j^2
    \label{Ridge_c}
\end{equation}

where $\lambda \in \{1,\dots,\infty\}$, is a tuning parameter that is to be determined by cross-validation. Cross-validation will be discussed later in the methodology. Minimizing the expression in \ref{Ridge_c}, gives the solution

\begin{equation}
    \hat{\beta}^{Ridge} = (\boldsymbol{X}^T\boldsymbol{X}+\lambda\boldsymbol{I})^{-1}\boldsymbol{X}^T\tilde{Z}.
\end{equation}

$\boldsymbol{I}$ is the identity-matrix and multiplied with $\lambda$, it creates a diagonal matrix that resolves any singularity of $\boldsymbol{X^T}\boldsymbol{X}$. Yet, Ridge regression still holds the disadvantage that it contains coefficients for all variables $p$, as it can only shrink their values towards, but not equal to zero. 


\subsubsection{Lasso Regression}
Lasso regression is a model that excludes some of the predictors by having the cost function in equation \ref{Lasso}.

\begin{equation}
    RSS + \lambda\sum_{j}^{p}\lvert\beta_j\rvert
    \label{Lasso}
\end{equation}


This cost function, on the contrary to the two prior-mentioned-ones, has not a simple analytical solution for its coefficients. Therefore, for the purpose of this study, a built-in function from Skicit-Learn will be used to evaluate Lasso-coefficients.  


\subsection{Resampling}
When using the models in the previous section to compute predictions, one wishes to obtain a fresh set of samples and evaluate how well the predictions perform. This is however not always possible, as data might be limited. Also, re-using the sample pairs of $Z$ and $(X = x, Y = y \dots)$ that have determined the model parameters, will give too promising results.


\subsubsection{K-fold Cross Validation}
A solution to the problem of limited data is cross-validation. Cross-validation divides the data into two groups and label them as either training-data and test-data. The training-data are used to fit the model parameters and the test-data are used to validate the corresponding fit these parameters generate. To compensate for any dependency of which parts of the data set that are labeled as test and train data, the K-fold resampling technique repeats the repeats the process as follows:

\begin{enumerate}
    \item Shuffle the data randomly 
    \item Divide the data into k number of equal-sized groups or "folds"
    \item Thereof, for every fold do:
    \begin{enumerate}
        \item Decide the group used for validation 
        \item Use the remaining folds as training data 
        \item Fit the model with the training data and evaluate its performance with the test-data
        \item Retain the statistical (to be defined later) scores and discard the model
        \end{enumerate}
    \item The cross-validation score is sets of statistical properties for the k-fold. Complete the k-fold method by averaging these. 
\end{enumerate}
    
The final, and perhaps most critical part of the algorithm is to choose the number of folds. Choosing too few folds give training sets that are quite similar and correlated, creating high variance in the expected test error. On the contrary, too many folds will give larger bias in the predictions as the training data are only representing small parts of the original training set. A good compromise is taking the number of folds equal to 5 or 10. For the purpose of this study, the number of folds are set to 5. 


\subsubsection{Bootstrap}
Bootstrap resampling is different from KFold as it resamples with replacement. The procedure is described by the following process

\begin{enumerate}
    \item Split the data-set into test and train data
    \item With the train data, for the number of straps selected, randomly draw a new set of independent variables to construct a surrogate train-data-set.  
    \item Use the surrogate train-data-set to construct model parameters and hence predictions for new observations.
    \item  Evaluate how well the predictions are relative to the test-data from point 1.  
\end{enumerate}

As the name cross validation suggests, its primary purpose is measuring (generalization) performance of a model. On contrast, bootstrapping is primarily used to establish empirical distribution functions for a widespread range of statistics (widespread as in ranging from, say,\newline
\newline

\noindent To compare, K-fold cross-validation is primarily used to measure model performance by estimating prediction errors, whereas bootstrap merely give standard errors of the predictors. Thus, for the purpose of this study, Kfold is the main resample technique used for model performance. Bootstrap is however included to illustrate the bias-variance relationship of MSE, which will be discussed further in detail under section 2.3.

\subsection{Statistical properties}

To better understand how well the different regression models are performing in the sense of predictions, the following statistical properties are used. 

\subsubsection{Variance}
The variance-covariance matrix of $\hat{\beta}$, can be found by the following expressions for OLS and Ridge regression respectively


\begin{equation}
    Var\left(\hat{\beta}_{OLS}\right) = (\boldsymbol{X^{T}X})^{-1}\sigma^2
    \label{var}
\end{equation}

\begin{equation}
    Var\left(\hat{\beta}_{Ridge}\right) = (\boldsymbol{X^{T}X}+\lambda_{diag.M})^{-1}\sigma^2
\end{equation}



An unbiased unbiased estimate for $\sigma$, can be found by 
\begin{equation}
    \hat{\sigma}^2 = \frac{1}{N-(p-1)}\sum_{i}^{N}(\tilde{z}_i-z_i)
    \label{sigm}
\end{equation}

Where $\tilde{z}_i$ is a single prediction for observation $z_i$, that comes from a pool of $N$ corresponding prediction-observations pairs $\tilde{Z}$ and $Z$. Similarly to prior sections, $p$ stands for the number of parameters. 
\newline

As the variance of the coefficients are the diagonal elements of $Var(\hat{\beta})$, equation \ref{var} and \ref{sigm} can be used to investigate confidence intervals of the coefficients $\hat{\beta}$. This is done by defining $STD_{\hat{\beta}}$ as the squared root of the variance and the expression

\begin{equation}
    \hat{\beta}{\pm}1.96\times{STD_{\hat{\beta}}}
\end{equation}

\subsubsection{MSE ans R2-score}
To evaluate the model performances, the statistical measures mean squared error, MSE, and R2-score are used. These are respectively expressed in the equations below.

\begin{equation}
    MSE = \frac{1}{N}\sum_i^N(z_i-\tilde{z}_i)
\end{equation}


\begin{equation}
    R2 = 1-\frac{\sum_i^N(z_i-\tilde{z}_i)}{\sum_i^N(z_i-\bar{z}_i)}
\end{equation}

MSE is simply the average value of the prediction errors, giving the most optimal score of zero. R2 on the other hand, which is a normalized version of MSE that measures how well the variance of observations are explained by the regression line, has the most optimal score of 1.   


\subsubsection{Bias-Variance Trade-off}
The MSE can be decomposed into three components that describes different prediction behaviour as seen below

\begin{align*}
    MSE &= E[(Z-\tilde{Z})^2] =  E[\tilde{Z}^2]-2\times{E[\tilde{Z}]}E[Z]+E[Z^2] = E[(\tilde{Z}-\bar{\tilde{Z}})^2]+\bar{\tilde{Z}}^2-2\times(\bar{\tilde{Z}}f(\boldsymbol{X}))+E[(y-f(\boldsymbol{X}))^2]+f(\boldsymbol{X})^2 \\
    &= E[(\tilde{Z}-\bar{\tilde{Z}})^2]+(\tilde{Z}-f(\boldsymbol{X}))^2+E[(Z-f(\boldsymbol{X}))^2] = Var(\tilde{Z})+Bias(\tilde{Z})^2+E[\sigma^2])
\end{align*}


Where the lemma 

\begin{equation}
     E[Z] = \frac{1}{N}\sum_i^N(Z) = \bar{z_i}
\end{equation}
\begin{equation}
    E[Z^2] = E[(Z-\bar{Z)}^2]+\bar{Z}
\end{equation}


is used. The decomposition gives a description of how the model complexity changes the different sources of the error. With a high model complexity one expect that the prediction bias will decrease, but unfortunately, the variance of the on the other hand will increase. That is, the certain points of prediction will give a better fit relative to the observations, but the predictions will be much more fluctuating as the sampled points are changed. 



\section{Results}



\subsection{Franke Function}

The Franke function is described by the following equation

\begin{align*}
f(x,y) &= \frac{3}{4}\exp{\left(-\frac{(9x-2)^2}{4} - \frac{(9y-2)^2}{4}\right)}+\frac{3}{4}\exp{\left(-\frac{(9x+1)^2}{49}- \frac{(9y+1)}{10}\right)} \\
&+\frac{1}{2}\exp{\left(-\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4}\right)} -\frac{1}{5}\exp{\left(-(9x-4)^2 - (9y-7)^2\right) }.
\end{align*}


Figure \ref{OLs_Lasso_Ridge} illustrates the MSEs for the various models with increasing degree of model complexity are illustrated. The Franke-function has in all these scenarios a normally distributed added noise with mean and variance equal to 0 and 1 respectively. In the bottom two plot, the MSE is also adjusted for the tuning parameter $\lambda$. Results of the lowest MSE-scores from the models are also listed in table \ref{table_MSE}. It should be noted that the Lasso model in some of the algorithm-runs had issues with converging leading to some difficulties in obtaining the absolute optimal $\lambda$ Nevertheless,it is clear that the OLS performance is superior to the others, creating the best fitted curve for the Franke-function. The rather low statistical scores are due to the noise being relativley large to the tre Franke-function. A general R2-score in the order of 0.07, determines that the fit of the models are just slightly better that of the horizontal hyper-plane. 



\begin{table}[h]
\centering
\caption{Results Franke Function}
\begin{tabular}{lll}
\textbf{Method} & \textbf{MSE} & \textbf{R2}  \\
 OLS(Deg = 5) & 1.0444 & 0.0748 \\
 Ridge(Deg = 4, $\lambda = 10^{-6}$) &  1.0501   &  0.0713  \\
 Lasso (Deg = 5, $\lambda = 10^{-3})$ & 1.0511  &  0.0719 
 \label{table_MSE}
\end{tabular}
\end{table}


\begin{figure}[h]
    \centering
    \includegraphics[width=9cm,height=\textheight,keepaspectratio]{Figures/OLS_MSE_final.png}
    \includegraphics[width=9cm,height=\textheight,keepaspectratio]{Figures/Ridge_franke1.png}
    \includegraphics[width=9cm,height=\textheight,keepaspectratio]{Figures/Lasso_Franke1.png}
    \caption[MSE for increasing model complexity of OLS.] {The results are after cross-validation using 5 folded KFold
    \textit{Top:} OLS \textit{Middle:} Ridge \textit{Bottom:} Lasso}
   \label{OLs_Lasso_Ridge}
\end{figure}

\clearpage
\noident

\noindent Further, by evaluating the 95$\%$ confidence intervals of the models, it is clear that the tuning parameter $\lambda$ indeed decrease the size of the $\beta{s}$. These result are listed in the tables \ref{first}-\ref{last}, where there are parameters from Ridge-regression that both include and exclude intercept fitting. For the remainder of the results in this paper, the intercept is excluded from penalization and hence shrinkage in both Lasso and Ridge analysis. The behaviours of beta as a function of the tuning parameter $\lambda$ are also shown in figure \ref{Betaplot}. There is an obvious distinction between Ridge and Lasso as the coefficients of Ridge never reaches zero. The disruptive behaviour of Lasso where the model parameters increase or decrease again after reaching zero,  is due to the model not converging for certain levels of $\lambda$. 



\begin{figure}[h!]
    \centering
    \includegraphics[width=8cm,height=\textheight,keepaspectratio]{Figures/beta_ridge.png}
    \includegraphics[width=8cm,height=\textheight,keepaspectratio]{Figures/beta_lasso.png}
    \caption[Beta As function of $\lambda$]{The plots show the development of beta relative to $lambda$
    \textit{Top:} Ridge \textit{Bottom:} Lasso}
   \label{Betaplot}
\end{figure}




\noident Dividing the data into train and test samples, performing Kfold cross-validation and studying the development of model MSE, derives the bias-variance trade-off as seen by figure \ref{TT}. Although the scaling being slightly different, there are not surprisingly lower spreads of the coefficient-penalizing models. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=8cm,height=\textheight,keepaspectratio]{Figures/OLS_franke.png}
    \includegraphics[width=8cm,height=\textheight,keepaspectratio]{Figures/Ridge_TT_No_Intercept.png}
    \includegraphics[width=8cm,height=\textheight,keepaspectratio]{Figures/Lasso_TT_USE.png}
    \caption[Bias-Variance-Trade Off]{The plots show MSEs for train and test data used in fitting the Noisy Franke-function. The 
    \textit{Top:} OLS \textit{Middle:} Ridge $\lambda = 10^{-5}$ \textit{Bottom:} Lasso $\lambda = 10^{-3}$}
   \label{TT}
\end{figure}

\clearpage

\noident Lower train-test spreads are also illustrated by bias-variance decomposition of MSE in figure \ref{BV}. For OLS, there is a clear dependency of the variance to the polynomial degree of the fit, and the added noise factor $\sigma$. This phenomenon arises from the models over-fitting the noise instead of the general behaviour of the true Franke-function function. Evaluating the other model plots, there is a significant lower dependency of the variance, giving a better expected prediction-performance for higher degrees of polynomial fit. 
 



\begin{figure}[h!]
    \centering
    \includegraphics[width=7cm,height=\textheight,keepaspectratio]{Figures/OLS_BV_low_noise.png}
    \includegraphics[width=7cm,height=\textheight,keepaspectratio]{Figures/Ridge_BV.png}
    \includegraphics[width=7cm,height=\textheight,keepaspectratio]{Figures/OLS_BV}
    \includegraphics[width=7cm,height=\textheight,keepaspectratio]{Figures/Lasso_BV.png}
    \caption[Bias-Variance-Trade Off]{The plots show the decomposition of bias and variance for OLS fitting the Noisy Franke-function. The number of data points N = 2500. To generate the plot, bootstrap cross-validation with 100 straps is used. \textit{Top Left:} OLS, $\sigma$ = 0.1 \textit{Bottom Left:} OLS, $\sigma=1$\textit{Top Right:} Ridge, $\sigma$ = 1 \textit{Bottom Right:} Lasso, $\sigma= 1$}
   \label{BV}
\end{figure}

\clearpage

\subsection{Real Terrain Data}

To widen the perspective of the model performances, they have also been applied to real terrain data from the west coast of Norway. The data used are imported from a geoTiff-file that originates from a "SRTM 1 arc-second global" radar scan. The resolution of the terrain data are about 1 point for every 30 meters. To avoid extensive time-consumption on running the models on the complete terrain file, a random subset-sample of size $60\times{60}$ is chosen. This sample size is assumed to give a fair resolution that does not omit critical information about the terrain, maintaining a moderate computational time-consumption.
\newline
\newline
\noident In table \ref{terrain_table}, one can see that the Ridge model gives best statistical scores, and hence the most adequate fit. It should however be noted that the result are obtained by normalizing the data in order to reach faster convergence of the Lasso model. This reduces the overall MSE of the models drastically, but retain the relative performance. 

\begin{table}[h]
\centering
\caption{Results Terrain}
\begin{tabular}{lll}
\textbf{Method} & \textbf{MSE} & \textbf{R2}  \\
 OLS(Deg = 6) & 0.0034 & 0.9551 \\
 Ridge(Deg = 10, $\lambda = 10^{-6}$) &  0.0015   &  0.9804  \\
 Lasso (Deg = 3, $\lambda = 10^{-2})$ &  0.0144  &  0.8127 
\label{terrain_table}
\end{tabular}
\end{table}



\begin{figure}[h!]
    \centering
    \includegraphics[width=8cm,height=\textheight,keepaspectratio]{Figures/OLS}
    \includegraphics[width=8cm,height=\textheight,keepaspectratio]{Figures/Ridge_TT_No_Intercept.png}
    \includegraphics[width=8cm,height=\textheight,keepaspectratio]{Figures/Lasso_TT_USE.png}
    \caption[Bias-Variance-Trade Off]{The plots show MSEs for train and test data used in fitting the Noisy Franke-function. The 
    \textit{Top:} OLS \textit{Middle:} Ridge $\lambda = 10^{-5}$ \textit{Bottom:} Lasso $\lambda = 10^{-3}$}
   \label{TT}
\end{figure}







\clearpage






Normalisering - Subtrack smallest value 

hvordan og hvordan dette pårviker resultatene 




\section{Conclusion}

\begin{table}[h]
    \centering
    \caption{$\textbf{OLS - Franke Function:}$ Beta 95 $\%$ Confidence Intervals, $\sigma = 0$}
    \begin{tabular}{rrrr}
    \hline
    \\
    $\beta_{1}$ & 0.4399 & $\pm$ & 0.026686 \\
    $\beta_{2}$ & 7.192611 & $\pm$ & 0.31035 \\
    $\beta_{3}$ & 3.608336 & $\pm$ & 0.31035 \\
    $\beta_{4}$ & -30.87819 & $\pm$ & 1.545341 \\
    $\beta_{5}$ & -14.36734 & $\pm$ & 1.203517 \\
    $\beta_{6}$ & -7.999469 & $\pm$ & 1.545341 \\
    $\beta_{7}$ & 40.423473 & $\pm$ & 3.561038 \\
    $\beta_{8}$ & 42.008736 & $\pm$ & 2.629228 \\
    $\beta_{9}$ & 20.977651 & $\pm$ & 2.629228 \\
    $\beta_{10}$ & -9.827762 & $\pm$ & 3.561038 \\
    $\beta_{11}$ & -15.349568 & $\pm$ & 3.8028 \\
    $\beta_{12}$ & -50.951684 & $\pm$ & 2.892316 \\
    $\beta_{13}$ & -6.347269 & $\pm$ & 2.687421 \\
    $\beta_{14}$ & -31.291939 & $\pm$ & 2.892316 \\
    $\beta_{15}$ & 31.504667 & $\pm$ & 3.8028 \\
    $\beta_{16}$ & -1.826569 & $\pm$ & 1.521711 \\
    $\beta_{17}$ & 18.44583 & $\pm$ & 1.318104 \\
    $\beta_{18}$ & 9.475388 & $\pm$ & 1.283242 \\
    $\beta_{19}$ & -5.215509 & $\pm$ & 1.283242 \\
    $\beta_{20}$ & 17.583493 & $\pm$ & 1.318104 \\
    $\beta_{21}$ & -17.569517 & $\pm$ & 1.521711 \\
    \hline
    \end{tabular}

    \label{first}
\end{table}

\begin{table}[h]
    \centering
    \caption{$\textbf{OLS - Franke Function:}$ Beta 95$\%$ Confidence Intervals, $\sigma = 1$}
    \begin{tabular}{rrrr}
    \hline
    \\
    $\beta_{1}$ & 0.921113 & $\pm$ & 0.588935 \\
    $\beta_{2}$ & -0.574779 & $\pm$ & 6.849254 \\
    $\beta_{3}$ & 1.315243 & $\pm$ & 6.849254 \\
    $\beta_{4}$ & 3.819495 & $\pm$ & 34.104849 \\
    $\beta_{5}$ & 6.237675 & $\pm$ & 26.560989 \\
    $\beta_{6}$ & -0.468507 & $\pm$ & 34.104849 \\
    $\beta_{7}$ & -24.101804 & $\pm$ & 78.590213 \\
    $\beta_{8}$ & -11.257645 & $\pm$ & 58.025652 \\
    $\beta_{9}$ & 5.767721 & $\pm$ & 58.025652 \\
    $\beta_{10}$ & -30.409204 & $\pm$ & 78.590213 \\
    $\beta_{11}$ & 35.781807 & $\pm$ & 83.925776 \\
    $\beta_{12}$ & 9.66368 & $\pm$ & 63.831882 \\
    $\beta_{13}$ & 13.39685 & $\pm$ & 59.309941 \\
    $\beta_{14}$ & -31.362884 & $\pm$ & 63.831882 \\
    $\beta_{15}$ & 59.460871 & $\pm$ & 83.925776 \\
    $\beta_{16}$ & -15.733603 & $\pm$ & 33.583351 \\
    $\beta_{17}$ & -7.566359 & $\pm$ & 29.089854 \\
    $\beta_{18}$ & 3.041943 & $\pm$ & 28.320466 \\
    $\beta_{19}$ & -7.8527 & $\pm$ & 28.320466 \\
    $\beta_{20}$ & 20.933326 & $\pm$ & 29.089854 \\
    $\beta_{21}$ & -30.901493 & $\pm$ & 33.583351 \\
    \hline
    \end{tabular}

    \label{tab:my_label}
\end{table}

\begin{table}[h]
    \centering
    \caption{$\textbf{Ridge (Intercept Included) - Franke Function:}$ Confidence Intervals, $\sigma = 0$, $\lambda = 10^{-5}$}
    \begin{tabular}{rrrr}
    \hline
    \\
    $\beta_{1}$ & 0.45178 & $\pm$ & 0.026425 \\
    $\beta_{2}$ & 6.988898 & $\pm$ & 0.305517 \\
    $\beta_{3}$ & 3.562249 & $\pm$ & 0.305517 \\
    $\beta_{4}$ & -29.947305 & $\pm$ & 1.516726 \\
    $\beta_{5}$ & -13.72819 & $\pm$ & 1.186623 \\
    $\beta_{6}$ & -8.064134 & $\pm$ & 1.516726 \\
    $\beta_{7}$ & 38.623387 & $\pm$ & 3.492924 \\
    $\beta_{8}$ & 40.441826 & $\pm$ & 2.594061 \\
    $\beta_{9}$ & 20.051926 & $\pm$ & 2.594061 \\
    $\beta_{10}$ & -9.292039 & $\pm$ & 3.492924 \\
    $\beta_{11}$ & -13.793259 & $\pm$ & 3.733327 \\
    $\beta_{12}$ & -49.28668 & $\pm$ & 2.859305 \\
    $\beta_{13}$ & -5.359876 & $\pm$ & 2.657807 \\
    $\beta_{14}$ & -30.563383 & $\pm$ & 2.859305 \\
    $\beta_{15}$ & 30.694982 & $\pm$ & 3.733327 \\
    $\beta_{16}$ & -2.323573 & $\pm$ & 1.496376 \\
    $\beta_{17}$ & 17.816776 & $\pm$ & 1.307338 \\
    $\beta_{18}$ & 9.064781 & $\pm$ & 1.274623 \\
    $\beta_{19}$ & -5.465432 & $\pm$ & 1.274623 \\
    $\beta_{20}$ & 17.34377 & $\pm$ & 1.307338 \\
    $\beta_{21}$ & -17.19427 & $\pm$ & 1.496376 \\
    \hline
    \end{tabular}

    \label{tab:my_label}
\end{table}


\begin{table}[h]
    \centering
    \caption{$\textbf{Ridge (Intercept Included) - Franke Function:}$ Confidence Intervals, $\sigma = 1$, $\lambda = 10^{-5}$}
    \begin{tabular}{rrrr}
    \hline
    \\
    $\beta_{1}$ & 0.914857 & $\pm$ & 0.583094 \\
    $\beta_{2}$ & -0.475519 & $\pm$ & 6.74146 \\
    $\beta_{3}$ & 1.405175 & $\pm$ & 6.74146 \\
    $\beta_{4}$ & 3.224114 & $\pm$ & 33.467724 \\
    $\beta_{5}$ & 6.131757 & $\pm$ & 26.183752 \\
    $\beta_{6}$ & -1.090746 & $\pm$ & 33.467724 \\
    $\beta_{7}$ & -22.698071 & $\pm$ & 77.074061 \\
    $\beta_{8}$ & -10.817492 & $\pm$ & 57.239952 \\
    $\beta_{9}$ & 5.735971 & $\pm$ & 57.239952 \\
    $\beta_{10}$ & -28.684862 & $\pm$ & 77.07406 \\
    $\beta_{11}$ & 34.340072 & $\pm$ & 82.378737 \\
    $\beta_{12}$ & 9.130171 & $\pm$ & 63.09278 \\
    $\beta_{13}$ & 13.218182 & $\pm$ & 58.646556 \\
    $\beta_{14}$ & -31.198114 & $\pm$ & 63.09278 \\
    $\beta_{15}$ & 57.451627 & $\pm$ & 82.378737 \\
    $\beta_{16}$ & -15.194761 & $\pm$ & 33.018696 \\
    $\beta_{17}$ & -7.353913 & $\pm$ & 28.847423 \\
    $\beta_{18}$ & 3.155272 & $\pm$ & 28.125552 \\
    $\beta_{19}$ & -7.847482 & $\pm$ & 28.125552 \\
    $\beta_{20}$ & 20.847883 & $\pm$ & 28.847423 \\
    $\beta_{21}$ & -30.074743 & $\pm$ & 33.018696 \\
    \hline
    \end{tabular}

    \label{tab:my_label}
\end{table}



\begin{table}[h]
    \centering
    \caption{$\textbf{Ridge (Intercept Omitted) - Franke Function:}$ Confidence Intervals, $\sigma = 0$, $\lambda = 10^{-5}$}
    \begin{tabular}{rrrr}
    \hline
    \\
    $\beta_{1}$ & 10.717053 & $\pm$ & 0.031677 \\
    $\beta_{2}$ & 7.305338 & $\pm$ & 0.36624 \\
    $\beta_{3}$ & -42.010015 & $\pm$ & 0.36624 \\
    $\beta_{4}$ & -28.992907 & $\pm$ & 1.818183 \\
    $\beta_{5}$ & -20.200198 & $\pm$ & 1.42247 \\
    $\beta_{6}$ & 57.399821 & $\pm$ & 1.818183 \\
    $\beta_{7}$ & 66.487322 & $\pm$ & 4.18716 \\
    $\beta_{8}$ & 46.176438 & $\pm$ & 3.109643 \\
    $\beta_{9}$ & 9.618026 & $\pm$ & 3.109643 \\
    $\beta_{10}$ & -27.866235 & $\pm$ & 4.18716 \\
    $\beta_{11}$ & -69.794131 & $\pm$ & 4.475344 \\
    $\beta_{12}$ & -27.95755 & $\pm$ & 3.427607 \\
    $\beta_{13}$ & -51.179849 & $\pm$ & 3.186059 \\
    $\beta_{14}$ & 16.516132 & $\pm$ & 3.427607 \\
    $\beta_{15}$ & 1.756825 & $\pm$ & 4.475344 \\
    $\beta_{16}$ & 23.947743 & $\pm$ & 1.793789 \\
    $\beta_{17}$ & 16.180055 & $\pm$ & 1.567178 \\
    $\beta_{18}$ & 1.662972 & $\pm$ & 1.527961 \\
    $\beta_{19}$ & 23.519894 & $\pm$ & 1.527961 \\
    $\beta_{20}$ & -13.083343 & $\pm$ & 1.567178 \\
    \hline
    \end{tabular}

    \label{tab:my_label}
\end{table}






\begin{table}[h]
    \centering
    \caption{$\textbf{Ridge (Intercept Omitted - Franke Function:}$ Confidence Intervals, $\sigma = 1$, $\lambda = 10^{-5}$}
    \begin{tabular}{rrrr}
    \hline
    \\
    $\beta_{1}$ & 7.069875 & $\pm$ & 0.584214 \\
    $\beta_{2}$ & 8.987233 & $\pm$ & 6.754417 \\
    $\beta_{3}$ & -21.171506 & $\pm$ & 6.754417 \\
    $\beta_{4}$ & -24.781735 & $\pm$ & 33.532046 \\
    $\beta_{5}$ & -25.67174 & $\pm$ & 26.234075 \\
    $\beta_{6}$ & 15.246427 & $\pm$ & 33.532046 \\
    $\beta_{7}$ & 41.907285 & $\pm$ & 77.222192 \\
    $\beta_{8}$ & 58.649619 & $\pm$ & 57.349963 \\
    $\beta_{9}$ & 9.607595 & $\pm$ & 57.349963 \\
    $\beta_{10}$ & 5.921633 & $\pm$ & 77.222191 \\
    $\beta_{11}$ & -32.364008 & $\pm$ & 82.537063 \\
    $\beta_{12}$ & -32.541442 & $\pm$ & 63.21404 \\
    $\beta_{13}$ & -72.954497 & $\pm$ & 58.759271 \\
    $\beta_{14}$ & 28.748819 & $\pm$ & 63.21404 \\
    $\beta_{15}$ & -6.960835 & $\pm$ & 82.537063 \\
    $\beta_{16}$ & 5.04577 & $\pm$ & 33.082156 \\
    $\beta_{17}$ & 17.557301 & $\pm$ & 28.902866 \\
    $\beta_{18}$ & 6.589599 & $\pm$ & 28.179607 \\
    $\beta_{19}$ & 33.355751 & $\pm$ & 28.179607 \\
    $\beta_{20}$ & -21.755926 & $\pm$ & 28.902866 \\
    \hline
    \end{tabular}

    \label{tab:my_label}
\end{table}

\begin{table}[h]
    \centering
    \caption{$\textbf{Lasso - Franke Function:}$ Confidence Intervals, $\sigma = 0$, $\lambda = 0.001$}
    \begin{tabular}{rrrr}
    \hline
    \\
    $\beta_{1}$ & 0.989098 & $\pm$ & 0.3903 \\
    $\beta_{2}$ & -0.558396 & $\pm$ & 0.3934 \\
    $\beta_{3}$ & 0.0 & $\pm$ & 0.3895 \\
    $\beta_{4}$ & -0.235655 & $\pm$ & 0.3911 \\
    $\beta_{5}$ & 0.044902 & $\pm$ & 0.3943 \\
    $\beta_{6}$ & -1.314648 & $\pm$ & 0.3929 \\
    $\beta_{7}$ & 0.0 & $\pm$ & 0.3904 \\
    $\beta_{8}$ & 0.594329 & $\pm$ & 0.3907 \\
    $\beta_{9}$ & 0.0 & $\pm$ & 0.3938 \\
    $\beta_{10}$ & 0.0 & $\pm$ & 0.3899 \\
    $\beta_{11}$ & 0.0 & $\pm$ & 0.3952 \\
    $\beta_{12}$ & 0.0 & $\pm$ & 0.3916 \\
    $\beta_{13}$ & 0.0 & $\pm$ & 0.3904 \\
    $\beta_{14}$ & 0.0 & $\pm$ & 0.3926 \\
    $\beta_{15}$ & 0.0 & $\pm$ & 0.3895 \\
    $\beta_{16}$ & 0.0 & $\pm$ & 0.3935 \\
    $\beta_{17}$ & 0.0 & $\pm$ & 0.3936 \\
    $\beta_{18}$ & 0.0 & $\pm$ & 0.3901 \\
    $\beta_{19}$ & 0.0 & $\pm$ & 0.3926 \\
    $\beta_{20}$ & 0.0 & $\pm$ & 0.3905 \\
    $\beta_{21}$ & 0.601606 & $\pm$ & 0.393 \\
    \hline
    \end{tabular}

    \label{tab:my_label}
\end{table}



\begin{table}[h]
    \centering
    \caption{$\textbf{Lasso - Franke Function:}$ Confidence Intervals, $\sigma = 1$, $\lambda = 0.001$}
    \begin{tabular}{rrrr}
    \hline
    \\
    $\beta_{1}$ & 1.019569 & $\pm$ & 0.4113 \\
    $\beta_{2}$ & -0.39026 & $\pm$ & 0.4105 \\
    $\beta_{3}$ & 0.037923 & $\pm$ & 0.4192 \\
    $\beta_{4}$ & -0.549136 & $\pm$ & 0.4084 \\
    $\beta_{5}$ & 0.199539 & $\pm$ & 0.4045 \\
    $\beta_{6}$ & -1.398167 & $\pm$ & 0.4196 \\
    $\beta_{7}$ & 0.0 & $\pm$ & 0.4118 \\
    $\beta_{8}$ & 0.0 & $\pm$ & 0.4082 \\
    $\beta_{9}$ & 0.0 & $\pm$ & 0.4255 \\
    $\beta_{10}$ & 0.0 & $\pm$ & 0.4252 \\
    $\beta_{11}$ & 0.0 & $\pm$ & 0.409 \\
    $\beta_{12}$ & 0.214776 & $\pm$ & 0.4454 \\
    $\beta_{13}$ & 0.0 & $\pm$ & 0.4314 \\
    $\beta_{14}$ & 0.0 & $\pm$ & 0.4244 \\
    $\beta_{15}$ & 0.0 & $\pm$ & 0.3977 \\
    $\beta_{16}$ & 0.0 & $\pm$ & 0.4058 \\
    $\beta_{17}$ & 0.510878 & $\pm$ & 0.4301 \\
    $\beta_{18}$ & 0.0 & $\pm$ & 0.3938 \\
    $\beta_{19}$ & 0.0 & $\pm$ & 0.4199 \\
    $\beta_{20}$ & 0.0 & $\pm$ & 0.3957 \\
    $\beta_{21}$ & 0.531661 & $\pm$ & 0.4027 \\
    \hline
    \end{tabular}

    \label{last}
\end{table}



%degOLS = 4
%degRidge = 4
%degLasso = 5
%lambRidge = 10**-6
%lambLasso = 10**-3

%MSE OLS: 1.0444409416329086
%R2 OLS: 0.07477273925035846
%MSE Ridge: 1.0500634725712572
%R2 Ridge: 0.07131812269996249
%MSE Lasso: 1.05112250039866
%R2 Lasso: 0.07191586482254479
%\begin{table}[]
%\centering
%\caption{Results Franke Function}
%\begin{tabular}{lll}
%\textbf{Method} & \textbf{MSE} & \textbf{R2}  \\
% OLS(Deg = 5) & 1.0444 & 0.0748 \\
% Ridge(Deg = 4, $\lambda = 10^{-6}$) &  1.0501   &  0.0713  \\
% Lasso (Deg = 5, $\lambda = 10^{-3})$ & 1.0511  &  0.0719 
%\end{tabular}
%\end{table}







\end{document}
